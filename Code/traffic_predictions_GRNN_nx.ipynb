{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc từng file từ thư mục MXH_Dataset\n",
    "train_df = pd.read_csv(\"../Dataset/train.csv\")\n",
    "segment_status_df = pd.read_csv(\"../Dataset/segment_status.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_status: <bound method DataFrame.info of          _id                updated_at  segment_id  velocity\n",
      "0          0  2020-07-03T14:55:31.869Z       24845        20\n",
      "1          1  2020-07-03T15:02:56.048Z       33923        10\n",
      "2          2  2020-07-04T08:15:52.696Z       33824         5\n",
      "3          3  2020-07-04T08:15:59.903Z       33824         5\n",
      "4          4  2020-07-04T08:16:08.201Z       33824         5\n",
      "...      ...                       ...         ...       ...\n",
      "90933  90933  2021-04-22T06:52:39.280Z       52247         1\n",
      "90934  90934  2021-04-22T06:52:52.501Z       52247         1\n",
      "90935  90935  2021-04-22T06:53:02.335Z       52247         1\n",
      "90936  90936  2021-04-22T06:53:14.294Z       52247         1\n",
      "90937  90937  2021-04-22T06:53:27.300Z       52247         1\n",
      "\n",
      "[90938 rows x 4 columns]>\n",
      "train: <bound method DataFrame.info of          _id  segment_id        date  weekday        period LOS   s_node_id  \\\n",
      "0          0          26  2021-04-16        4   period_0_30   A   366428456   \n",
      "1          1          33  2020-08-02        6  period_23_30   C   366469460   \n",
      "2          2          33  2020-08-03        0   period_0_00   D   366469460   \n",
      "3          3          67  2021-03-09        1   period_9_30   B   366403668   \n",
      "4          4          67  2021-03-23        1   period_9_30   B   366403668   \n",
      "...      ...         ...         ...      ...           ...  ..         ...   \n",
      "33436  33436       84529  2021-01-05        1   period_8_00   F   411918572   \n",
      "33437  33437       84533  2021-01-09        5  period_13_30   F  4597281310   \n",
      "33438  33438       84533  2021-04-22        3   period_5_00   F  4597281310   \n",
      "33439  33439       84534  2021-01-09        5  period_13_30   D   411918528   \n",
      "33440  33440       84535  2021-01-09        5  period_13_30   F  5140843585   \n",
      "\n",
      "        e_node_id  length  street_id  max_velocity  street_level  \\\n",
      "0       366416066     116   32575820           NaN             4   \n",
      "1      3792257828      26   32575862           NaN             3   \n",
      "2      3792257828      26   32575862           NaN             3   \n",
      "3      5755066033       7   32575862           NaN             3   \n",
      "4      5755066033       7   32575862           NaN             3   \n",
      "...           ...     ...        ...           ...           ...   \n",
      "33436  1226517262      13  654864528           NaN             2   \n",
      "33437   411918528     126  654864530           NaN             4   \n",
      "33438   411918528     126  654864530           NaN             4   \n",
      "33439   411925985      89  654864530           NaN             4   \n",
      "33440  6150226443     139  656562463           NaN             4   \n",
      "\n",
      "                street_name   street_type  long_snode  lat_snode  long_enode  \\\n",
      "0             Nguyễn Văn Bá      tertiary  106.768732  10.841506  106.769254   \n",
      "1                Đường số 5     secondary  106.761957  10.878650  106.762143   \n",
      "2                Đường số 5     secondary  106.761957  10.878650  106.762143   \n",
      "3                Đường số 5     secondary  106.768412  10.880817  106.768461   \n",
      "4                Đường số 5     secondary  106.768412  10.880817  106.768461   \n",
      "...                     ...           ...         ...        ...         ...   \n",
      "33436  Nguyễn Thị Minh Khai       primary  106.704503  10.789614  106.704584   \n",
      "33437     Nguyễn Bỉnh Khiêm      tertiary  106.704986  10.787267  106.704149   \n",
      "33438     Nguyễn Bỉnh Khiêm      tertiary  106.704986  10.787267  106.704149   \n",
      "33439     Nguyễn Bỉnh Khiêm      tertiary  106.704149  10.788051  106.703549   \n",
      "33440                   NaN  unclassified  106.828885  10.694142  106.827709   \n",
      "\n",
      "       lat_enode  \n",
      "0      10.842422  \n",
      "1      10.878808  \n",
      "2      10.878808  \n",
      "3      10.880771  \n",
      "4      10.880771  \n",
      "...          ...  \n",
      "33436  10.789702  \n",
      "33437  10.788051  \n",
      "33438  10.788051  \n",
      "33439  10.788604  \n",
      "33440  10.693657  \n",
      "\n",
      "[33441 rows x 18 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra số dòng, số cột của từng file\n",
    "for name, df in [(\"segment_status\", segment_status_df), \n",
    "                 (\"train\", train_df)]:\n",
    "    print(f\"{name}: {df.info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Xử lý dữ liệu thiếu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train_df:\n",
      " _id                 0\n",
      "segment_id          0\n",
      "date                0\n",
      "weekday             0\n",
      "period              0\n",
      "LOS                 0\n",
      "s_node_id           0\n",
      "e_node_id           0\n",
      "length              0\n",
      "street_id           0\n",
      "max_velocity    28495\n",
      "street_level        0\n",
      "street_name         1\n",
      "street_type         0\n",
      "long_snode          0\n",
      "lat_snode           0\n",
      "long_enode          0\n",
      "lat_enode           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in train_df:\\n\", train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train_df:\n",
      " _id             0\n",
      "segment_id      0\n",
      "date            0\n",
      "weekday         0\n",
      "period          0\n",
      "LOS             0\n",
      "s_node_id       0\n",
      "e_node_id       0\n",
      "length          0\n",
      "street_id       0\n",
      "max_velocity    0\n",
      "street_level    0\n",
      "street_name     0\n",
      "street_type     0\n",
      "long_snode      0\n",
      "lat_snode       0\n",
      "long_enode      0\n",
      "lat_enode       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Xử lý giá trị thiếu\n",
    "train_df['max_velocity'] = train_df['max_velocity'].fillna(train_df['max_velocity'].mean())\n",
    "train_df['street_name'] = train_df['street_name'].fillna('Unknown')\n",
    "\n",
    "print(\"Missing values in train_df:\\n\", train_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi thời gian\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "segment_status_df['updated_at'] = pd.to_datetime(segment_status_df['updated_at'])\n",
    "train_df = train_df.sort_values('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chuẩn hóa cột LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_df['LOS_encoded'] = le.fit_transform(train_df['LOS'])\n",
    "scaler = MinMaxScaler()\n",
    "train_df['LOS_norm'] = scaler.fit_transform(train_df[['LOS_encoded']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tạo đồ thị với NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11452\\3941005010.py\", line 11, in <module>\n",
      "    edge_index_torch = torch.tensor(edge_index[['s_node_idx', 'e_node_idx']].T.values, dtype=torch.int64)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11452\\3941005010.py:11: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  edge_index_torch = torch.tensor(edge_index[['s_node_idx', 'e_node_idx']].T.values, dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "edge_index = train_df[['s_node_id', 'e_node_id', 'segment_id']].drop_duplicates().dropna().astype(int)\n",
    "\n",
    "# Tạo ánh xạ cho node IDs\n",
    "node_ids = sorted(set(edge_index['s_node_id']).union(edge_index['e_node_id']))\n",
    "node_id_to_index = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
    "num_nodes = len(node_ids)\n",
    "\n",
    "# Cập nhật edge_index với chỉ số mới\n",
    "edge_index['s_node_idx'] = edge_index['s_node_id'].map(node_id_to_index)\n",
    "edge_index['e_node_idx'] = edge_index['e_node_id'].map(node_id_to_index)\n",
    "edge_index_torch = torch.tensor(edge_index[['s_node_idx', 'e_node_idx']].T.values, dtype=torch.int64)\n",
    "\n",
    "# Thêm cạnh từ edge_index\n",
    "G = nx.DiGraph()\n",
    "for _, row in edge_index.iterrows():\n",
    "    G.add_edge(row['s_node_id'], row['e_node_id'], segment_id=row['segment_id'])\n",
    "\n",
    "# Tạo đồ thị với NetworkX\n",
    "G = nx.DiGraph()\n",
    "for _, row in edge_index.iterrows():\n",
    "    G.add_edge(row['s_node_idx'], row['e_node_idx'], segment_id=row['segment_id'])\n",
    "\n",
    "# Tạo đặc trưng nút\n",
    "node_features = []\n",
    "for node in range(num_nodes):  # Lặp qua chỉ số từ 0 đến num_nodes-1\n",
    "    degree = G.degree(node)\n",
    "    connected_edges = [(u, v) for u, v, d in G.edges(data=True) if u == node or v == node]\n",
    "    segment_ids = [G[u][v]['segment_id'] for u, v in connected_edges]\n",
    "    if segment_ids:\n",
    "        segment_data = train_df[train_df['segment_id'].isin(segment_ids)]\n",
    "        avg_length = segment_data['length'].mean() if 'length' in segment_data else 0\n",
    "        avg_velocity = segment_data['velocity'].mean() if 'velocity' in segment_data else 0\n",
    "    else:\n",
    "        avg_length, avg_velocity = 0, 0\n",
    "    node_features.append([degree, avg_length, avg_velocity])\n",
    "\n",
    "node_features = torch.tensor(node_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Merge 2 file segment_status và train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm convert period (vd: \"period_14_30\") thành timedelta\n",
    "def period_to_time(period_str):\n",
    "    try:\n",
    "        _, hour_str, min_str = period_str.split(\"_\")\n",
    "        hour = int(hour_str)\n",
    "        minute = int(min_str)\n",
    "        return pd.to_timedelta(f\"{hour}:{minute}:00\")\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply và tạo cột thời gian đầy đủ\n",
    "train_df['time_delta'] = train_df['period'].apply(period_to_time)\n",
    "train_df['date'] = pd.to_datetime(train_df['date']) + train_df['time_delta']\n",
    "\n",
    "# Xoá cột phụ nếu muốn\n",
    "train_df.drop(columns='time_delta', inplace=True)\n",
    "\n",
    "# Chuyển 'date' và 'updated_at' về datetime\n",
    "train_df['date'] = pd.to_datetime(train_df['date']).dt.tz_localize(None)\n",
    "segment_status_df['updated_at'] = pd.to_datetime(segment_status_df['updated_at']).dt.tz_localize(None)\n",
    "\n",
    "# Sort trước khi dùng merge_asof\n",
    "train = train_df.sort_values(by='date')\n",
    "segment_status = segment_status_df.sort_values(by='updated_at')\n",
    "\n",
    "# Merge gần đúng theo thời gian, trong cùng segment_id\n",
    "merged_df = pd.merge_asof(\n",
    "    train,\n",
    "    segment_status,\n",
    "    by='segment_id',\n",
    "    left_on='date',\n",
    "    right_on='updated_at',\n",
    "    direction='nearest',  # hoặc 'backward' nếu bạn chỉ muốn dùng dữ liệu trước đó\n",
    "    tolerance=pd.Timedelta('30min')  # chỉ chấp nhận khớp nếu lệch thời gian <= 30 phút\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Tạo pivot table cho LOS_norm và velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11452\\947608898.py:7: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  ).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).sort_index(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11452\\947608898.py:15: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  ).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).sort_index(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Tạo los_pivot\n",
    "los_pivot = merged_df.pivot_table(\n",
    "    index='segment_id',\n",
    "    columns='date',\n",
    "    values='LOS_norm',\n",
    "    aggfunc='mean'\n",
    ").fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).sort_index(axis=1)\n",
    "\n",
    "# Tạo velocity_pivot từ merged_df\n",
    "velocity_pivot = merged_df.pivot_table(\n",
    "    index='segment_id',\n",
    "    columns='date',\n",
    "    values='velocity',\n",
    "    aggfunc='mean'\n",
    ").fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Tạo danh sách snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of snapshots: 827\n",
      "Edge index shape: torch.Size([2, 10026])\n",
      "Edge features shape: torch.Size([10026, 2])\n",
      "Node features shape: torch.Size([11314, 3])\n",
      "Targets shape: torch.Size([10026, 1])\n"
     ]
    }
   ],
   "source": [
    "# Tạo edge_index từ NetworkX\n",
    "edge_index_torch = torch.tensor(list(G.edges(data=False)), dtype=torch.int64).t()\n",
    "\n",
    "# Tạo danh sách snapshot\n",
    "snapshots = []\n",
    "for date in los_pivot.columns:\n",
    "    edge_features = []\n",
    "    targets = []\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        segment_id = d['segment_id']\n",
    "        if segment_id in los_pivot.index:\n",
    "            los = los_pivot.loc[segment_id, date]\n",
    "            velocity = velocity_pivot.loc[segment_id, date]\n",
    "            edge_features.append([los, velocity])\n",
    "            targets.append([los])\n",
    "        else:\n",
    "            edge_features.append([0, 0])\n",
    "            targets.append([0])\n",
    "    edge_features = torch.tensor(edge_features, dtype=torch.float32)\n",
    "    targets = torch.tensor(targets, dtype=torch.float32)\n",
    "    snapshots.append({\n",
    "        'edge_index': edge_index_torch,\n",
    "        'edge_features': edge_features,\n",
    "        'node_features': node_features,\n",
    "        'targets': targets\n",
    "    })\n",
    "\n",
    "# Kiểm tra snapshot\n",
    "print(f\"Number of snapshots: {len(snapshots)}\")\n",
    "print(f\"Edge index shape: {snapshots[0]['edge_index'].shape}\")\n",
    "print(f\"Edge features shape: {snapshots[0]['edge_features'].shape}\")\n",
    "print(f\"Node features shape: {snapshots[0]['node_features'].shape}\")\n",
    "print(f\"Targets shape: {snapshots[0]['targets'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Định nghĩa mô hình GRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Định nghĩa mô hình GRNN\n",
    "class GRNN(nn.Module):\n",
    "    def __init__(self, node_features_dim, edge_features_dim, hidden_dim, out_dim):\n",
    "        super(GRNN, self).__init__()\n",
    "        self.conv = nn.Linear(2 * node_features_dim + edge_features_dim, hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, snapshots):\n",
    "        outputs = []\n",
    "        h = None\n",
    "        for snapshot in snapshots:\n",
    "            edge_index = snapshot['edge_index']  # [2, num_edges]\n",
    "            edge_features = snapshot['edge_features']  # [num_edges, edge_features_dim]\n",
    "            node_features = snapshot['node_features']  # [num_nodes, node_features_dim]\n",
    "\n",
    "            # Kết hợp đặc trưng nút và cạnh\n",
    "            src_features = node_features[edge_index[0]]  # [num_edges, node_features_dim]\n",
    "            dst_features = node_features[edge_index[1]]  # [num_edges, node_features_dim]\n",
    "            edge_input = torch.cat([src_features, dst_features, edge_features], dim=1)\n",
    "\n",
    "            # Áp dụng convolution\n",
    "            h_edge = torch.relu(self.conv(edge_input))  # [num_edges, hidden_dim]\n",
    "\n",
    "            # Tổng hợp đặc trưng cạnh cho mỗi nút\n",
    "            node_h = torch.zeros(node_features.size(0), h_edge.size(1)).to(h_edge.device)\n",
    "            node_h.index_add_(0, edge_index[0], h_edge)\n",
    "            node_h.index_add_(0, edge_index[1], h_edge)\n",
    "\n",
    "            # RNN\n",
    "            node_h = node_h.unsqueeze(0)  # [1, num_nodes, hidden_dim]\n",
    "            node_h, h = self.rnn(node_h, h)\n",
    "            node_h = node_h.squeeze(0)  # [num_nodes, hidden_dim]\n",
    "\n",
    "            # Dự đoán cho mỗi cạnh\n",
    "            edge_out = self.fc(h_edge)  # [num_edges, out_dim]\n",
    "            outputs.append(edge_out)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = GRNN(node_features_dim=3, edge_features_dim=2, hidden_dim=32, out_dim=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6277169371434818\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Xử lý từng snapshot\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msnapshot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Gọi model với danh sách 1 snapshot\u001b[39;00m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, snapshot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Huấn luyện\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for snapshot in snapshots:\n",
    "        optimizer.zero_grad()\n",
    "        # Xử lý từng snapshot\n",
    "        output = model([snapshot])[0]  # Gọi model với danh sách 1 snapshot\n",
    "        loss = criterion(output, snapshot['targets'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(snapshots)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    outputs = model(snapshots)\n",
    "    for out in outputs:\n",
    "        predictions.append(out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá\n",
    "targets = [snapshot['targets'].cpu().numpy() for snapshot in snapshots]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(np.concatenate(targets), np.concatenate(predictions))\n",
    "print(f\"Test MSE: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
